{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.3-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python393jvsc74a57bd0b0a99d1505f268174e5bf63147f81569413db5fe994fbb23454e5bc5b3cee597",
      "display_name": "Python 3.9.3 64-bit ('.venv')"
    },
    "colab": {
      "name": "2020mt93713_RohitVarghese.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitvarghese96/AI_LearningAgents/blob/main/2020mt93713_RohitVarghese.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh0ozafBktS0"
      },
      "source": [
        "<strong>AI Assignment</strong> <br/>\n",
        "Student Name: Rohit Varghese <br/>\n",
        "StudentID: 2020mt93713"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPpxxiPqktS7"
      },
      "source": [
        "<strong>About Learning Agent</strong><br/>\n",
        "\n",
        "<p>This learning agent is based on the Q-Learning concept which requires no model. The agent selects action independently of its environement. The state that the agent will end up in after taking an action is known only to the environment itself.</p>\n",
        "\n",
        "<br/><br/>\n",
        "\n",
        "<strong>Q-Function</strong>\n",
        "Q-learning agent performs actions and receives rewards for those actions, the corresponding entry in the Q-table is updated using the function, known as the Q-function, below:\n",
        "\n",
        "$$Q^{new}(s_t,\\:a_t) \\leftarrow Q(s_t,\\:a_t) \\times (1 - \\alpha) + \\alpha(r_t + \\gamma \\times \\underset{a}{max}\\:Q(s_{t+1},\\:a))$$\n",
        "\n",
        "where,<br/>\n",
        "$Q^{new}(s_t,\\:a_t)$: refers to a value in the Q-table, where st is the state at time t and at is the action taken at time t. <br/>\n",
        "\n",
        "α: is the learning rate where 0 ≤ α ≤ 1. The learning rate determines how much new information affects what the agent has already learned. <br/>\n",
        "\n",
        "rt: it is the reward received at time t after performing action at at state st. <br/>\n",
        "\n",
        "γ: it is the discount factor where 0 ≤ γ <1. This determines how much future rewards affect our decision making. As the discount factor approaches 1, the agent will pay more attention to which actions will lead to the highest future reward. If the discount factor is 0, then the agent will only care about immediate rewards. <br/>\n",
        "\n",
        "$\\underset{a}{max}\\:Q(s_{t+1},\\:a))$: it  is the estimate of the optimal future value. st+1 is the state we end up in after taking action at at state st. max(...) means that we take the greatest value of Q(st+1,a) for all available actions a at state st+1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onQ4Yk7OktS9"
      },
      "source": [
        "#Therefore, based on the above concept, we are now setting the appropriate learning parameters\n",
        "epsilon = 0.9\n",
        "discountFactor = 0.9\n",
        "learningRate = 0.9\n",
        "totalEpisodes = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ofSx9KAktS9"
      },
      "source": [
        "#import libraries\n",
        "import numpy as np\n",
        "\n",
        "#define environment\n",
        "environment_rows = 10\n",
        "envrironment_columns = 10\n",
        "\n",
        "#define agent starting location\n",
        "start_row = 9\n",
        "start_column = 0\n",
        "\n",
        "#initial Battery Points\n",
        "initialBatteryPoints = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIsZM4CAktS_"
      },
      "source": [
        "#Initialize Q-Table and its environment\n",
        "\n",
        "#define possible actions\n",
        "actions = ['up', 'down', 'left', 'right']\n",
        "\n",
        "#creating 3d numpr array holding the current Q-Values for each state and action pair\n",
        "q_values = np.zeros((environment_rows, envrironment_columns, 4))\n",
        "rewards = np.full((environment_rows, envrironment_columns), -1)\n",
        "\n",
        "def initializeEnvironment():\n",
        "    #creating 3d numpr array holding the current Q-Values for each state and action pair\n",
        "    global q_values\n",
        "    q_values = np.zeros((environment_rows, envrironment_columns, 4))\n",
        "\n",
        "\n",
        "    global rewards \n",
        "    rewards = np.full((environment_rows, envrironment_columns), -1)\n",
        "\n",
        "    rewards[9, 9] = 10000 #setting the reward for gold\n",
        "\n",
        "    #set rewards for metal sensing locations\n",
        "    #row0\n",
        "    rewards[0, 7] = -100\n",
        "    rewards[0, 8] = -100\n",
        "    rewards[0, 9] = -100\n",
        "\n",
        "\n",
        "    #row1\n",
        "    rewards[1, 2] = -100\n",
        "    rewards[1, 3] = -100\n",
        "    rewards[1, 4] = -100\n",
        "    rewards[1, 7] = -100\n",
        "    rewards[1, 8] = -100\n",
        "    rewards[1, 9] = -100\n",
        "\n",
        "\n",
        "    #row2\n",
        "    rewards[2, 2] = -100\n",
        "    rewards[2, 3] = -100\n",
        "    rewards[2, 4] = -100\n",
        "    rewards[2, 6] = -100\n",
        "    rewards[2, 7] = -100\n",
        "    rewards[2, 8] = -100\n",
        "    rewards[2, 9] = -100\n",
        "\n",
        "    #row3\n",
        "    rewards[3, 0] = -100\n",
        "    rewards[3, 1] = -100\n",
        "    rewards[3, 2] = -100\n",
        "    rewards[3, 3] = -100\n",
        "    rewards[3, 4] = -100\n",
        "    rewards[3, 6] = -100\n",
        "    rewards[3, 7] = -100\n",
        "    rewards[3, 8] = -100\n",
        "\n",
        "    #row4\n",
        "    rewards[4, 0] = -100\n",
        "    rewards[4, 1] = -100\n",
        "    rewards[4, 2] = -100\n",
        "    rewards[4, 3] = -100\n",
        "    rewards[4, 4] = -100\n",
        "    rewards[4, 6] = -100\n",
        "    rewards[4, 7] = -100\n",
        "    rewards[4, 8] = -100\n",
        "\n",
        "    #row5\n",
        "    rewards[5, 0] = -100\n",
        "    rewards[5, 1] = -100\n",
        "    rewards[5, 2] = -100\n",
        "    rewards[5, 3] = -100\n",
        "    rewards[5, 4] = -100\n",
        "\n",
        "    #row6\n",
        "    rewards[6, 2] = -100\n",
        "    rewards[6, 3] = -100\n",
        "    rewards[6, 4] = -100\n",
        "\n",
        "    #row7\n",
        "    rewards[7, 6] = -100\n",
        "    rewards[7, 7] = -100\n",
        "    rewards[7, 8] = -100\n",
        "\n",
        "    #row8\n",
        "    rewards[8, 1] = -100\n",
        "    rewards[8, 2] = -100\n",
        "    rewards[8, 3] = -100\n",
        "    rewards[8, 6] = -100\n",
        "    rewards[8, 7] = -100\n",
        "    rewards[8, 8] = -100\n",
        "\n",
        "    #row9\n",
        "    rewards[9, 1] = -100\n",
        "    rewards[9, 2] = -100\n",
        "    rewards[9, 3] = -100\n",
        "    rewards[9, 6] = -100\n",
        "    rewards[9, 7] = -100\n",
        "    rewards[9, 8] = -100\n",
        "\n",
        "initializeEnvironment()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jinQ6zzGktTA"
      },
      "source": [
        "#Defining control functions\n",
        "#function that determines if given location is a terminal state\n",
        "def isTerminalState(currentRow, currentColumn):\n",
        "    if rewards[currentRow, currentColumn] == -1:\n",
        "        return False\n",
        "\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "\n",
        "\n",
        "#define next action based on epsilon greedy algorithm\n",
        "def getNextAction(currentRow, currentColumn, epsilon):\n",
        "    #if random value is less than epsilon, then produce next action based on the greedy-algorithm\n",
        "    #else, return random action\n",
        "    if np.random.random() < epsilon:\n",
        "        return np.argmax(q_values[currentRow, currentColumn])\n",
        "    else:\n",
        "        return np.random.randint(4)\n",
        "\n",
        "\n",
        "\n",
        "#function that will get next location based on chosen action\n",
        "def getNextLocation(currentRow, currentColumn, actionIndex):\n",
        "    newRow = currentRow\n",
        "    newColumn = currentColumn\n",
        "\n",
        "    if actions[actionIndex] == 'up' and currentRow < environment_rows - 1:\n",
        "        newRow += 1\n",
        "\n",
        "    elif actions[actionIndex] == 'down' and currentRow > 0:\n",
        "        newRow -= 1\n",
        "\n",
        "    elif actions[actionIndex] == 'left' and currentColumn > 0:\n",
        "        newColumn -= 1\n",
        "\n",
        "    elif actions[actionIndex] == 'right' and currentColumn < envrironment_columns - 1:\n",
        "        newColumn += 1\n",
        "    return newRow, newColumn\n",
        "\n",
        "\n",
        "\n",
        "#function that will return the optimal path\n",
        "def getOptimalPath(startRow, startColumn):\n",
        "    batteryPoints = initialBatteryPoints\n",
        "\n",
        "    if isTerminalState(startRow, startColumn):\n",
        "        return []\n",
        "\n",
        "    else:\n",
        "        currentRow, currentColumn = startRow, startColumn\n",
        "        optimalPath = []\n",
        "        optimalPath.append([currentRow, currentColumn])\n",
        "\n",
        "        #continue until gold is found\n",
        "        while not isTerminalState(currentRow, currentColumn):\n",
        "            #get next best action\n",
        "            actionIndex = getNextAction(currentRow, currentColumn, 1.)\n",
        "\n",
        "            #move to next location\n",
        "            currentRow, currentColumn = getNextLocation(currentRow, currentColumn, actionIndex)\n",
        "\n",
        "            reward = rewards[currentRow, currentColumn]\n",
        "            batteryPoints = batteryPoints + reward\n",
        "\n",
        "            #append the path\n",
        "            optimalPath.append([currentRow, currentColumn])\n",
        "\n",
        "        return optimalPath, batteryPoints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP-5okWtktTB"
      },
      "source": [
        "#Training the learning agent\n",
        "def trainLearningAgent():\n",
        "    for episode in range(totalEpisodes):\n",
        "        currentRow = start_row\n",
        "        currentColumn = start_column\n",
        "        batteryPoints = initialBatteryPoints\n",
        "\n",
        "        #continue taking actions while not in terminal state\n",
        "        while not isTerminalState(currentRow, currentColumn) and batteryPoints>0:\n",
        "            #choose which action to take\n",
        "            actionIndex = getNextAction(currentRow, currentColumn, epsilon)\n",
        "\n",
        "            #perform the next action\n",
        "            oldRow, oldColumn = currentRow, currentColumn\n",
        "            currentRow, currentColumn = getNextLocation(currentRow, currentColumn, actionIndex)\n",
        "\n",
        "            #recieve the reward for moving to next state\n",
        "            reward = rewards[currentRow, currentColumn]\n",
        "            batteryPoints = batteryPoints + reward\n",
        "            old_qValue = q_values[oldRow, oldColumn, actionIndex]\n",
        "            temporalDifference = reward + (discountFactor * np.max(q_values[currentRow, currentColumn])) - old_qValue\n",
        "\n",
        "            #update q-value for previous state and action pair\n",
        "            new_qValue = old_qValue + (learningRate * temporalDifference)\n",
        "            q_values[oldRow, oldColumn, actionIndex] = new_qValue\n",
        "\n",
        "trainLearningAgent()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8W0CrVRktTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18c012dd-c8c8-4abd-a158-c0bd3cad0728"
      },
      "source": [
        "#get optimal path\n",
        "optimalPath, batteryPoints = getOptimalPath(start_row, start_column)\n",
        "print(\"Optimal Path: \" + str(optimalPath))\n",
        "print(\"Total Battery Points: \" + str(batteryPoints))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal Path: [[9, 0], [8, 0], [7, 0], [7, 1], [7, 2], [7, 3], [7, 4], [7, 5], [6, 5], [6, 6], [6, 7], [6, 8], [6, 9], [7, 9], [8, 9], [9, 9]]\n",
            "Total Battery Points: 19986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Z-N5NyvktTC"
      },
      "source": [
        "After running multiple learning episodes, the learning agent will eventually find the below optimal path maximizing its reward points\n",
        "<br/>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=10pBGcJlp1S-VoauRHHa5VwCz7jFqfu2a\" width=500 height=500/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNC_O3UkktTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53829dba-ac6e-43dd-c3fd-93cded61ad24"
      },
      "source": [
        "#Adjusting the learning parameters\n",
        "\n",
        "#Store the number of steps of each learning set to find quick convergence\n",
        "numberOfSteps = []\n",
        "\n",
        "#No:1\n",
        "epsilon = 0.8\n",
        "discountFactor = 0.8\n",
        "learningRate = 0.8\n",
        "totalEpisodes = 900\n",
        "\n",
        "#Reset the environment\n",
        "initializeEnvironment()\n",
        "\n",
        "#train the leraning agent\n",
        "trainLearningAgent()\n",
        "\n",
        "#get the optimal path and append its reward point\n",
        "optimalPath, batteryPoints = getOptimalPath(start_row, start_column)\n",
        "numberOfSteps.append(len(optimalPath))\n",
        "\n",
        "\n",
        "#No:2\n",
        "epsilon = 0.7\n",
        "discountFactor = 0.7\n",
        "learningRate = 0.7\n",
        "totalEpisodes = 600\n",
        "\n",
        "#Reset the environment\n",
        "initializeEnvironment()\n",
        "\n",
        "#train the leraning agent\n",
        "trainLearningAgent()\n",
        "\n",
        "#get the optimal path and append its reward point\n",
        "optimalPath, batteryPoints = getOptimalPath(start_row, start_column)\n",
        "numberOfSteps.append(len(optimalPath))\n",
        "\n",
        "\n",
        "\n",
        "#No:3\n",
        "epsilon = 0.65\n",
        "discountFactor = 0.8\n",
        "learningRate = 0.6\n",
        "totalEpisodes = 800\n",
        "\n",
        "#Reset the environment\n",
        "initializeEnvironment()\n",
        "\n",
        "#train the leraning agent\n",
        "trainLearningAgent()\n",
        "\n",
        "#get the optimal path and append its reward point\n",
        "optimalPath, batteryPoints = getOptimalPath(start_row, start_column)\n",
        "numberOfSteps.append(len(optimalPath))\n",
        "\n",
        "\n",
        "\n",
        "#No:4\n",
        "epsilon = 0.6\n",
        "discountFactor = 0.62\n",
        "learningRate = 0.64\n",
        "totalEpisodes = 1000\n",
        "\n",
        "#Reset the environment\n",
        "initializeEnvironment()\n",
        "\n",
        "#train the leraning agent\n",
        "trainLearningAgent()\n",
        "\n",
        "#get the optimal path and append its reward point\n",
        "optimalPath, batteryPoints = getOptimalPath(start_row, start_column)\n",
        "numberOfSteps.append(len(optimalPath))\n",
        "\n",
        "\n",
        "\n",
        "#No:5\n",
        "epsilon = 0.63\n",
        "discountFactor = 0.7\n",
        "learningRate = 0.63\n",
        "totalEpisodes = 900\n",
        "\n",
        "#Reset the environment\n",
        "initializeEnvironment()\n",
        "\n",
        "#train the leraning agent\n",
        "trainLearningAgent()\n",
        "\n",
        "#get the optimal path and append its reward point\n",
        "optimalPath, batteryPoints = getOptimalPath(start_row, start_column)\n",
        "numberOfSteps.append(len(optimalPath))\n",
        "\n",
        "\n",
        "#Get quickest set\n",
        "quickestSet = numberOfSteps.index(min(numberOfSteps)) + 1\n",
        "print(\"Quickest Learning Set: \" + str(quickestSet))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Quickest Learning Set: 1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}